{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编写第一个网络爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "抓取网站数据，我们首先得下载包含有感兴趣数据的网页，这个过程称之为爬取（crawling）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬取一个网站方法有很多，而哪种网站更合适取决于我们所选择网站的结构。本文中我们首先会探讨如何安全地下载网页，然后会介绍如下3种爬取网站的常见方法：   \n",
    "爬取网站地图  \n",
    "使用数据库ID遍历每个网页  \n",
    "跟踪网页链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来让我们先定义这两种方法的相似点和不同点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、抓取与爬取的对比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据我们所关注的信息和站点内容、结构的不同，可能需要进行网络抓取或是网站爬取。\n",
    "\n",
    "那么它们有什么区别呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络抓取通常针对特定网站，并在这些站点上获取指定信息。网络抓取用于访问这些特定的页面，如果站点发生变化或者站点中的信息位置发生变化的话，则需要进行修改。例如，你可能想要通过网络抓取查看你喜欢的当地餐厅的每日特色菜，为了实现该目的，你需要抓取其网站中日常更新该信息的部分。  \n",
    "与之不同的是，网络爬取通常是以通用的方式构建的，其目标是一系列顶级域名的网站或是整个网络。爬取可以用来收集更具体的信息，不过更常见的情况是爬取网络，从许多不同的站点或页面中获取小而通用的信息，然后跟综链接到其他页面中。  \n",
    "除了爬取和抓取外，我们还会在第8章中介绍网络爬虫。爬虫可以用来爬取指定的一系列网站，或是在多个站点移至整个互联网中送行更广泛的爬取。  \n",
    "一般来说，我们会使用特定的术语反映我们的用例。在你开发网络爬虫时，可能会注意到它们在你想要使用的技术、库和包中的区别。在这些情况下，你对不同术语的理解，可以帮助你基于所使用的术语选择适当的包或技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如：是否只用于抓取？是否也适用于爬虫？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、下载网页"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们要想抓取网页的话，首先需要将其下载下来。示例脚本使用urllib模块下载URL。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "def download(url):\n",
    "    return urllib.request.urlopen(url).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "传入URL参数时，该函数将会下载网页并返回其HTML。不过，这个代码片段存在一个问题，当我们下载网页时，可能会遇到一些无法控制的错误，比如请求的页面可能不存在。这个时候urllib会抛出异常，然后退出脚本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安全起见，下面再给出一个更稳建的版本，可以捕获这些异常。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import URLError,HTTPError,ContentTooShortError\n",
    "\n",
    "def download(url):\n",
    "    print('Downloading:',url)\n",
    "    try:\n",
    "        html = urllib.request.urlopen(url).read()\n",
    "    except (URLError,HTTPError,ContentTooShortError) as e:\n",
    "        print('Download error:',e.reason)\n",
    "        html = None\n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，当出现下载或URL错误时，该函数能够捕获到异常，然后返回None。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、重试下载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在下载时遇到的错误一般都是临时性的，例如服务器过载时返回的503 Service Unavailable错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于此类错误，我们可以在短暂等待后尝试重新下载，因为这个服务器问题现在可能已经解决。不过，我们不需要对所有错误都尝试重新下载。如果服务器返回的是404 Not Found这种惜误，则说明该网页目前并不存在，再次尝试同样的请求一般也不会出现不同的结果。  \n",
    "互联网工程任务组(lnternet Engineering Task Force)定义了HTTP错误的完整列表，从中可以了解到4xx错误发生在请求存在问题时，而5xx错误则发生在服务端存在问题时。所以，我们只需要确保download函数在发生5xx错误时重试下载即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是支持重试下载功能的新版本代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url,num_returies = 2):\n",
    "    print('Downloading:',url)\n",
    "    try:\n",
    "        html = urllib.request.urlopen(url).read()\n",
    "    except (URLError,HTTPError,ContentTooShortError) as e:\n",
    "        print('Download error:',e.reason)\n",
    "        html = None\n",
    "        if num_returies > 0:\n",
    "            if hasattr(e,'code') and 500 <= e.code <600:\n",
    "                return download(url,num_returies - 1)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，在download函数遇到5xx错误码时，会递归调用函数本身来重试。该函数还增加了一个参数，用于设定重试下载的次数，默认为两次。之所以在这里限制网页下载尝试次数，可能是服务器错误暂时还木有恢复。想要测这个该函数，可以尝试下载http://httpstat.us/500 ，这个网址会始终返回500错误码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://httpstat.us/500\n",
      "Download error: Internal Server Error\n",
      "Downloading: http://httpstat.us/500\n",
      "Download error: Internal Server Error\n",
      "Downloading: http://httpstat.us/500\n",
      "Download error: Internal Server Error\n"
     ]
    }
   ],
   "source": [
    "download('http://httpstat.us/500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从返回的结果可以看出，download函数的行为和预期一致，先尝试下载网页，在接收到500错误后，又进行了两次重试才放弃。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、设置用户代理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在默认情况下urllib使用Python-urllib/``3.x作为用户代理下载网页内容，3.x是正在使用的Python版本号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果能使用可辨识的用户代理则更好，这样可以避免我们的网络爬虫碰到一些问题。此外，也许是因为曾经历过质量不佳的Python网络爬虫造成的服务器过载，一些网站还会封禁这个默认的用户代理。   \n",
    "因此，为了使下载网站更加可靠，我们需要控制用户代理的设定。下面的代码对download函数进行了修改，设定了一个默认的用户代理‘wswp’。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即Web Scraping with Python的首字母缩写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url,user_agent = 'wswp',num_returies = 2):\n",
    "    print('Downloading:',url)\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header('User-agent',user_agent)\n",
    "    try:\n",
    "        html = urllib.request.urlopen(url).read()\n",
    "    except (URLError,HTTPError,ContentTooShortError) as e:\n",
    "        print('Download error:',e.reason)\n",
    "        html = None\n",
    "        if num_returies > 0:\n",
    "            if hasattr(e,'code') and 500 <= e.code <600:\n",
    "                return download(url,num_returies - 1)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在再次访问meetup.com，就可以看到一个合法的HTML了。下载函数在后续代码中可以得到复用，这个函数能够捕获异常，在可能的情况下重试网站以及设置用户代理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、网站地图爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第一个简单的爬虫中，我们将使用示例网站robots.txt文件中发现的网站地图来下载所有网页。为了解析网站地图，就用一个简单的正则表达式，从<loc>标签中提取出URL。需要更新代码以处理编码转换，因为目前的download函数只是简单地返回了字节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def download(url,user_agent = 'wswp',num_retries = 2,charset = 'utf-8'):\n",
    "    print('Downloading:',url)\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header('User-agent',user_agent)\n",
    "    try:\n",
    "        resp = urllib.request.urlopen(request)\n",
    "        cs = resp.headers.get_content_charset()\n",
    "        if not cs:\n",
    "            cs = charset\n",
    "        html = resp.read().decode(cs)\n",
    "    except (URLError,HTTPError,ContentTooShortError) as e:\n",
    "        print('Download error:',e.reason)\n",
    "        html = None\n",
    "        if num_retries > 0:\n",
    "            if hasattr(e,'code') and 500 <= e.code < 600:\n",
    "                #recursively retury 5xx HTTP error\n",
    "                return download(url,num_retries - 1)\n",
    "    return html\n",
    "\n",
    "def crawl_sitemap(url):\n",
    "    #download the sitemap file\n",
    "    sitemap = download(url)\n",
    "    #extract the sitemap links\n",
    "    links = re.findall('<loc>(.*?)</loc>',sitemap)\n",
    "    #download each link\n",
    "    for link in links:\n",
    "        html = download(link)\n",
    "        #scrape html here\n",
    "        #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在运行网站地图爬虫，从示例网站中下载所有国家或地区页面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.python-scraping.com/sitemap.xml\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Afghanistan-1\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Aland-Islands-2\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Albania-3\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Algeria-4\n",
      "Downloading: http://example.python-scraping.com/places/default/view/American-Samoa-5\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Andorra-6\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Angola-7\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Anguilla-8\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Antarctica-9\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Antigua-and-Barbuda-10\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Argentina-11\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Armenia-12\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Aruba-13\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Australia-14\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Austria-15\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Azerbaijan-16\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Bahamas-17\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Bahrain-18\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Bangladesh-19\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Barbados-20\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Belarus-21\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Belgium-22\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Belize-23\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Benin-24\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Bermuda-25\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Bhutan-26\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Bolivia-27\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Bonaire-Saint-Eustatius-and-Saba-28\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Bosnia-and-Herzegovina-29\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Botswana-30\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Bouvet-Island-31\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Brazil-32\n",
      "Downloading: http://example.python-scraping.com/places/default/view/British-Indian-Ocean-Territory-33\n",
      "Downloading: http://example.python-scraping.com/places/default/view/British-Virgin-Islands-34\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Brunei-35\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Bulgaria-36\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Burkina-Faso-37\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Burundi-38\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Cambodia-39\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Cameroon-40\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Canada-41\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Cape-Verde-42\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Cayman-Islands-43\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Central-African-Republic-44\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Chad-45\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Chile-46\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Christmas-Island-47\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Cocos-Islands-48\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Colombia-49\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Comoros-50\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Cook-Islands-51\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Costa-Rica-52\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Croatia-53\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Cuba-54\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Curacao-55\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Cyprus-56\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Czech-Republic-57\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Democratic-Republic-of-the-Congo-58\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Denmark-59\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Djibouti-60\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Dominica-61\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Dominican-Republic-62\n",
      "Downloading: http://example.python-scraping.com/places/default/view/East-Timor-63\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Ecuador-64\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Egypt-65\n",
      "Downloading: http://example.python-scraping.com/places/default/view/El-Salvador-66\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Equatorial-Guinea-67\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Eritrea-68\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Estonia-69\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Ethiopia-70\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Faroe-Islands-71\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Fiji-72\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Finland-73\n",
      "Downloading: http://example.python-scraping.com/places/default/view/France-74\n",
      "Downloading: http://example.python-scraping.com/places/default/view/French-Guiana-75\n",
      "Downloading: http://example.python-scraping.com/places/default/view/French-Polynesia-76\n",
      "Downloading: http://example.python-scraping.com/places/default/view/French-Southern-Territories-77\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Gabon-78\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Gambia-79\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Georgia-80\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Germany-81\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Ghana-82\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Gibraltar-83\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Greece-84\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Greenland-85\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Grenada-86\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Guadeloupe-87\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Guam-88\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Guatemala-89\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Guernsey-90\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Guinea-91\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Guinea-Bissau-92\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Guyana-93\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Haiti-94\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Heard-Island-and-McDonald-Islands-95\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Honduras-96\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Hungary-97\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Iceland-98\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Indonesia-99\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Iran-100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.python-scraping.com/places/default/view/Iraq-101\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Ireland-102\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Isle-of-Man-103\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Israel-104\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Italy-105\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Ivory-Coast-106\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Jamaica-107\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Japan-108\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Jersey-109\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Jordan-110\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Kazakhstan-111\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Kenya-112\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Kiribati-113\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Kosovo-114\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Kuwait-115\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Kyrgyzstan-116\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Laos-117\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Latvia-118\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Lebanon-119\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Lesotho-120\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Liberia-121\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Libya-122\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Liechtenstein-123\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Lithuania-124\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Luxembourg-125\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Macedonia-126\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Madagascar-127\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Malawi-128\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Malaysia-129\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Maldives-130\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Mali-131\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Malta-132\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Marshall-Islands-133\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Martinique-134\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Mauritania-135\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Mauritius-136\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Mayotte-137\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Mexico-138\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Micronesia-139\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Moldova-140\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Monaco-141\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Mongolia-142\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Montenegro-143\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Montserrat-144\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Morocco-145\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Mozambique-146\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Myanmar-147\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Namibia-148\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Nauru-149\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Nepal-150\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Netherlands-151\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Netherlands-Antilles-152\n",
      "Downloading: http://example.python-scraping.com/places/default/view/New-Caledonia-153\n",
      "Downloading: http://example.python-scraping.com/places/default/view/New-Zealand-154\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Nicaragua-155\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Niger-156\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Nigeria-157\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Niue-158\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Norfolk-Island-159\n",
      "Downloading: http://example.python-scraping.com/places/default/view/North-Korea-160\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Northern-Mariana-Islands-161\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Norway-162\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Oman-163\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Pakistan-164\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Palau-165\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Palestinian-Territory-166\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Panama-167\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Papua-New-Guinea-168\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Paraguay-169\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Peru-170\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Philippines-171\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Pitcairn-172\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Poland-173\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Portugal-174\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Puerto-Rico-175\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Qatar-176\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Republic-of-the-Congo-177\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Reunion-178\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Romania-179\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Russia-180\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Rwanda-181\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Saint-Barthelemy-182\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Saint-Helena-183\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Saint-Kitts-and-Nevis-184\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Saint-Lucia-185\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Saint-Martin-186\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Saint-Pierre-and-Miquelon-187\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Saint-Vincent-and-the-Grenadines-188\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Samoa-189\n",
      "Downloading: http://example.python-scraping.com/places/default/view/San-Marino-190\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Sao-Tome-and-Principe-191\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Saudi-Arabia-192\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Senegal-193\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Serbia-194\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Serbia-and-Montenegro-195\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Seychelles-196\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Sierra-Leone-197\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Singapore-198\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Sint-Maarten-199\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Slovakia-200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.python-scraping.com/places/default/view/Slovenia-201\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Solomon-Islands-202\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Somalia-203\n",
      "Downloading: http://example.python-scraping.com/places/default/view/South-Africa-204\n",
      "Downloading: http://example.python-scraping.com/places/default/view/South-Georgia-and-the-South-Sandwich-Islands-205\n",
      "Downloading: http://example.python-scraping.com/places/default/view/South-Korea-206\n",
      "Downloading: http://example.python-scraping.com/places/default/view/South-Sudan-207\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Spain-208\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Sri-Lanka-209\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Sudan-210\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Suriname-211\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Svalbard-and-Jan-Mayen-212\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Swaziland-213\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Sweden-214\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Switzerland-215\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Syria-216\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Tajikistan-217\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Tanzania-218\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Thailand-219\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Togo-220\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Tokelau-221\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Tonga-222\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Trinidad-and-Tobago-223\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Tunisia-224\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Turkey-225\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Turkmenistan-226\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Turks-and-Caicos-Islands-227\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Tuvalu-228\n",
      "Downloading: http://example.python-scraping.com/places/default/view/US-Virgin-Islands-229\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Uganda-230\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Ukraine-231\n",
      "Downloading: http://example.python-scraping.com/places/default/view/United-Arab-Emirates-232\n",
      "Downloading: http://example.python-scraping.com/places/default/view/United-Kingdom-233\n",
      "Downloading: http://example.python-scraping.com/places/default/view/United-States-234\n",
      "Downloading: http://example.python-scraping.com/places/default/view/United-States-Minor-Outlying-Islands-235\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Uruguay-236\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Uzbekistan-237\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Vanuatu-238\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Vatican-239\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Venezuela-240\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Vietnam-241\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Wallis-and-Futuna-242\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Western-Sahara-243\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Yemen-244\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Zambia-245\n",
      "Downloading: http://example.python-scraping.com/places/default/view/Zimbabwe-246\n"
     ]
    }
   ],
   "source": [
    "crawl_sitemap('http://example.python-scraping.com/sitemap.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如上面代码中的download方法所示，我们必须更新字符编码才能利用正则表达式处理网站响应。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python的read方法返回字节，而正则表达式期望的则是字符串。我们的代码依赖于网站维护者在响应头中包含适当的字符编码。如果没有返回字符编码头部，我们将会把它设置为默认值UTF-8，并抱有最大的希望。当然，如果返回头中的编码不正确，或是编码没有设置并且也不是UTF-8的话，则会抛出错误。  \n",
    "还有一些更复杂的方式用于猜测编码(参见https://pypi.python.org/pypi/chardet )，该方法非常容易实现。到目前为止，网站地图爬虫已经符合预期。不过正如前文所述，我们无法依靠Sitemap文件提供每个网页的链接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一节中，我们将会介绍另一个简单的爬虫，该爬虫不再依赖于Sitemap文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、ID遍历爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节中利用网站结构的弱点，更加轻松地访问所有内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是一些示例国家(或地区)的URL  \n",
    "http://example.python-scraping.com/view/Afghanisten-1  \n",
    "http://example.python-scraping.com/view/Australia-2  \n",
    "http://example.python-scraping.com/view/Brazil-3  \n",
    "可以看出，这些URL只在URL路径的最后一部分有所区别，包括国家(或地区)名(作为页面别名)和ID.在URL中包含页面别名是非常普遍的做法，可以对搜索引擎优化起到帮助作用。一般情况下，Web服务器会忽略这个字符串，只使用ID来匹配数据库中的相关记录。下面我们将其移除，查看http://example.python-scraping.com/view/1 ，测试示例网站中的链接是否仍然可用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网页还是可以加载成功，也就是说这个方法是有用的。现在我们就可以忽略页面别名，只利用数据库ID来下载所有国家（或地区）的页面了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来看看使用了该技巧的代码片段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def crawl_site(url):\n",
    "    for page in itertools.count(1):\n",
    "        pg_url = '{}{}'.format(url,page)\n",
    "        html = download(pg_url)\n",
    "        if html is None:\n",
    "            break\n",
    "        #success - can scrape the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在可以使用该函数传入基础URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.python-scraping.com/view/-1\n",
      "Downloading: http://example.python-scraping.com/view/-2\n",
      "Downloading: http://example.python-scraping.com/view/-3\n",
      "Downloading: http://example.python-scraping.com/view/-4\n",
      "Downloading: http://example.python-scraping.com/view/-5\n",
      "Downloading: http://example.python-scraping.com/view/-6\n",
      "Downloading: http://example.python-scraping.com/view/-7\n",
      "Downloading: http://example.python-scraping.com/view/-8\n",
      "Downloading: http://example.python-scraping.com/view/-9\n",
      "Downloading: http://example.python-scraping.com/view/-10\n",
      "Downloading: http://example.python-scraping.com/view/-11\n",
      "Downloading: http://example.python-scraping.com/view/-12\n",
      "Downloading: http://example.python-scraping.com/view/-13\n",
      "Downloading: http://example.python-scraping.com/view/-14\n",
      "Downloading: http://example.python-scraping.com/view/-15\n",
      "Downloading: http://example.python-scraping.com/view/-16\n",
      "Downloading: http://example.python-scraping.com/view/-17\n",
      "Downloading: http://example.python-scraping.com/view/-18\n",
      "Downloading: http://example.python-scraping.com/view/-19\n",
      "Downloading: http://example.python-scraping.com/view/-20\n",
      "Downloading: http://example.python-scraping.com/view/-21\n",
      "Downloading: http://example.python-scraping.com/view/-22\n",
      "Downloading: http://example.python-scraping.com/view/-23\n",
      "Downloading: http://example.python-scraping.com/view/-24\n",
      "Downloading: http://example.python-scraping.com/view/-25\n",
      "Downloading: http://example.python-scraping.com/view/-26\n",
      "Downloading: http://example.python-scraping.com/view/-27\n",
      "Downloading: http://example.python-scraping.com/view/-28\n",
      "Downloading: http://example.python-scraping.com/view/-29\n",
      "Downloading: http://example.python-scraping.com/view/-30\n",
      "Downloading: http://example.python-scraping.com/view/-31\n",
      "Downloading: http://example.python-scraping.com/view/-32\n",
      "Downloading: http://example.python-scraping.com/view/-33\n",
      "Downloading: http://example.python-scraping.com/view/-34\n",
      "Downloading: http://example.python-scraping.com/view/-35\n",
      "Downloading: http://example.python-scraping.com/view/-36\n",
      "Downloading: http://example.python-scraping.com/view/-37\n",
      "Downloading: http://example.python-scraping.com/view/-38\n",
      "Downloading: http://example.python-scraping.com/view/-39\n",
      "Downloading: http://example.python-scraping.com/view/-40\n",
      "Downloading: http://example.python-scraping.com/view/-41\n",
      "Downloading: http://example.python-scraping.com/view/-42\n",
      "Downloading: http://example.python-scraping.com/view/-43\n",
      "Downloading: http://example.python-scraping.com/view/-44\n",
      "Downloading: http://example.python-scraping.com/view/-45\n",
      "Downloading: http://example.python-scraping.com/view/-46\n",
      "Downloading: http://example.python-scraping.com/view/-47\n",
      "Downloading: http://example.python-scraping.com/view/-48\n",
      "Downloading: http://example.python-scraping.com/view/-49\n",
      "Downloading: http://example.python-scraping.com/view/-50\n",
      "Downloading: http://example.python-scraping.com/view/-51\n",
      "Downloading: http://example.python-scraping.com/view/-52\n",
      "Downloading: http://example.python-scraping.com/view/-53\n",
      "Downloading: http://example.python-scraping.com/view/-54\n",
      "Downloading: http://example.python-scraping.com/view/-55\n",
      "Downloading: http://example.python-scraping.com/view/-56\n",
      "Downloading: http://example.python-scraping.com/view/-57\n",
      "Downloading: http://example.python-scraping.com/view/-58\n",
      "Downloading: http://example.python-scraping.com/view/-59\n",
      "Downloading: http://example.python-scraping.com/view/-60\n",
      "Downloading: http://example.python-scraping.com/view/-61\n",
      "Downloading: http://example.python-scraping.com/view/-62\n",
      "Downloading: http://example.python-scraping.com/view/-63\n",
      "Downloading: http://example.python-scraping.com/view/-64\n",
      "Downloading: http://example.python-scraping.com/view/-65\n",
      "Downloading: http://example.python-scraping.com/view/-66\n",
      "Downloading: http://example.python-scraping.com/view/-67\n",
      "Downloading: http://example.python-scraping.com/view/-68\n",
      "Downloading: http://example.python-scraping.com/view/-69\n",
      "Downloading: http://example.python-scraping.com/view/-70\n",
      "Downloading: http://example.python-scraping.com/view/-71\n",
      "Downloading: http://example.python-scraping.com/view/-72\n",
      "Downloading: http://example.python-scraping.com/view/-73\n",
      "Downloading: http://example.python-scraping.com/view/-74\n",
      "Downloading: http://example.python-scraping.com/view/-75\n",
      "Downloading: http://example.python-scraping.com/view/-76\n",
      "Downloading: http://example.python-scraping.com/view/-77\n",
      "Downloading: http://example.python-scraping.com/view/-78\n",
      "Downloading: http://example.python-scraping.com/view/-79\n",
      "Downloading: http://example.python-scraping.com/view/-80\n",
      "Downloading: http://example.python-scraping.com/view/-81\n",
      "Downloading: http://example.python-scraping.com/view/-82\n",
      "Downloading: http://example.python-scraping.com/view/-83\n",
      "Downloading: http://example.python-scraping.com/view/-84\n",
      "Downloading: http://example.python-scraping.com/view/-85\n",
      "Downloading: http://example.python-scraping.com/view/-86\n",
      "Downloading: http://example.python-scraping.com/view/-87\n",
      "Downloading: http://example.python-scraping.com/view/-88\n",
      "Downloading: http://example.python-scraping.com/view/-89\n",
      "Downloading: http://example.python-scraping.com/view/-90\n",
      "Downloading: http://example.python-scraping.com/view/-91\n",
      "Downloading: http://example.python-scraping.com/view/-92\n",
      "Downloading: http://example.python-scraping.com/view/-93\n",
      "Downloading: http://example.python-scraping.com/view/-94\n",
      "Downloading: http://example.python-scraping.com/view/-95\n",
      "Downloading: http://example.python-scraping.com/view/-96\n",
      "Downloading: http://example.python-scraping.com/view/-97\n",
      "Downloading: http://example.python-scraping.com/view/-98\n",
      "Downloading: http://example.python-scraping.com/view/-99\n",
      "Downloading: http://example.python-scraping.com/view/-100\n",
      "Downloading: http://example.python-scraping.com/view/-101\n",
      "Downloading: http://example.python-scraping.com/view/-102\n",
      "Downloading: http://example.python-scraping.com/view/-103\n",
      "Downloading: http://example.python-scraping.com/view/-104\n",
      "Downloading: http://example.python-scraping.com/view/-105\n",
      "Downloading: http://example.python-scraping.com/view/-106\n",
      "Downloading: http://example.python-scraping.com/view/-107\n",
      "Downloading: http://example.python-scraping.com/view/-108\n",
      "Downloading: http://example.python-scraping.com/view/-109\n",
      "Downloading: http://example.python-scraping.com/view/-110\n",
      "Downloading: http://example.python-scraping.com/view/-111\n",
      "Downloading: http://example.python-scraping.com/view/-112\n",
      "Downloading: http://example.python-scraping.com/view/-113\n",
      "Downloading: http://example.python-scraping.com/view/-114\n",
      "Downloading: http://example.python-scraping.com/view/-115\n",
      "Downloading: http://example.python-scraping.com/view/-116\n",
      "Downloading: http://example.python-scraping.com/view/-117\n",
      "Downloading: http://example.python-scraping.com/view/-118\n",
      "Downloading: http://example.python-scraping.com/view/-119\n",
      "Downloading: http://example.python-scraping.com/view/-120\n",
      "Downloading: http://example.python-scraping.com/view/-121\n",
      "Downloading: http://example.python-scraping.com/view/-122\n",
      "Downloading: http://example.python-scraping.com/view/-123\n",
      "Downloading: http://example.python-scraping.com/view/-124\n",
      "Downloading: http://example.python-scraping.com/view/-125\n",
      "Downloading: http://example.python-scraping.com/view/-126\n",
      "Downloading: http://example.python-scraping.com/view/-127\n",
      "Downloading: http://example.python-scraping.com/view/-128\n",
      "Downloading: http://example.python-scraping.com/view/-129\n",
      "Downloading: http://example.python-scraping.com/view/-130\n",
      "Downloading: http://example.python-scraping.com/view/-131\n",
      "Downloading: http://example.python-scraping.com/view/-132\n",
      "Downloading: http://example.python-scraping.com/view/-133\n",
      "Downloading: http://example.python-scraping.com/view/-134\n",
      "Downloading: http://example.python-scraping.com/view/-135\n",
      "Downloading: http://example.python-scraping.com/view/-136\n",
      "Downloading: http://example.python-scraping.com/view/-137\n",
      "Downloading: http://example.python-scraping.com/view/-138\n",
      "Downloading: http://example.python-scraping.com/view/-139\n",
      "Downloading: http://example.python-scraping.com/view/-140\n",
      "Downloading: http://example.python-scraping.com/view/-141\n",
      "Downloading: http://example.python-scraping.com/view/-142\n",
      "Downloading: http://example.python-scraping.com/view/-143\n",
      "Downloading: http://example.python-scraping.com/view/-144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.python-scraping.com/view/-145\n",
      "Downloading: http://example.python-scraping.com/view/-146\n",
      "Downloading: http://example.python-scraping.com/view/-147\n",
      "Downloading: http://example.python-scraping.com/view/-148\n",
      "Downloading: http://example.python-scraping.com/view/-149\n",
      "Downloading: http://example.python-scraping.com/view/-150\n",
      "Downloading: http://example.python-scraping.com/view/-151\n",
      "Downloading: http://example.python-scraping.com/view/-152\n",
      "Downloading: http://example.python-scraping.com/view/-153\n",
      "Downloading: http://example.python-scraping.com/view/-154\n",
      "Downloading: http://example.python-scraping.com/view/-155\n",
      "Downloading: http://example.python-scraping.com/view/-156\n",
      "Downloading: http://example.python-scraping.com/view/-157\n",
      "Downloading: http://example.python-scraping.com/view/-158\n",
      "Downloading: http://example.python-scraping.com/view/-159\n",
      "Downloading: http://example.python-scraping.com/view/-160\n",
      "Downloading: http://example.python-scraping.com/view/-161\n",
      "Downloading: http://example.python-scraping.com/view/-162\n",
      "Downloading: http://example.python-scraping.com/view/-163\n",
      "Downloading: http://example.python-scraping.com/view/-164\n",
      "Downloading: http://example.python-scraping.com/view/-165\n",
      "Downloading: http://example.python-scraping.com/view/-166\n",
      "Downloading: http://example.python-scraping.com/view/-167\n",
      "Downloading: http://example.python-scraping.com/view/-168\n",
      "Downloading: http://example.python-scraping.com/view/-169\n",
      "Downloading: http://example.python-scraping.com/view/-170\n",
      "Downloading: http://example.python-scraping.com/view/-171\n",
      "Downloading: http://example.python-scraping.com/view/-172\n",
      "Downloading: http://example.python-scraping.com/view/-173\n",
      "Downloading: http://example.python-scraping.com/view/-174\n",
      "Downloading: http://example.python-scraping.com/view/-175\n",
      "Downloading: http://example.python-scraping.com/view/-176\n",
      "Downloading: http://example.python-scraping.com/view/-177\n",
      "Downloading: http://example.python-scraping.com/view/-178\n",
      "Downloading: http://example.python-scraping.com/view/-179\n",
      "Downloading: http://example.python-scraping.com/view/-180\n",
      "Downloading: http://example.python-scraping.com/view/-181\n",
      "Downloading: http://example.python-scraping.com/view/-182\n",
      "Downloading: http://example.python-scraping.com/view/-183\n",
      "Downloading: http://example.python-scraping.com/view/-184\n",
      "Downloading: http://example.python-scraping.com/view/-185\n",
      "Downloading: http://example.python-scraping.com/view/-186\n",
      "Downloading: http://example.python-scraping.com/view/-187\n",
      "Downloading: http://example.python-scraping.com/view/-188\n",
      "Downloading: http://example.python-scraping.com/view/-189\n",
      "Downloading: http://example.python-scraping.com/view/-190\n",
      "Downloading: http://example.python-scraping.com/view/-191\n",
      "Downloading: http://example.python-scraping.com/view/-192\n",
      "Downloading: http://example.python-scraping.com/view/-193\n",
      "Downloading: http://example.python-scraping.com/view/-194\n",
      "Downloading: http://example.python-scraping.com/view/-195\n",
      "Downloading: http://example.python-scraping.com/view/-196\n",
      "Downloading: http://example.python-scraping.com/view/-197\n",
      "Downloading: http://example.python-scraping.com/view/-198\n",
      "Downloading: http://example.python-scraping.com/view/-199\n",
      "Downloading: http://example.python-scraping.com/view/-200\n",
      "Downloading: http://example.python-scraping.com/view/-201\n",
      "Downloading: http://example.python-scraping.com/view/-202\n",
      "Downloading: http://example.python-scraping.com/view/-203\n",
      "Downloading: http://example.python-scraping.com/view/-204\n",
      "Downloading: http://example.python-scraping.com/view/-205\n",
      "Downloading: http://example.python-scraping.com/view/-206\n",
      "Downloading: http://example.python-scraping.com/view/-207\n",
      "Downloading: http://example.python-scraping.com/view/-208\n",
      "Downloading: http://example.python-scraping.com/view/-209\n",
      "Downloading: http://example.python-scraping.com/view/-210\n",
      "Downloading: http://example.python-scraping.com/view/-211\n",
      "Downloading: http://example.python-scraping.com/view/-212\n",
      "Downloading: http://example.python-scraping.com/view/-213\n",
      "Downloading: http://example.python-scraping.com/view/-214\n",
      "Downloading: http://example.python-scraping.com/view/-215\n",
      "Downloading: http://example.python-scraping.com/view/-216\n",
      "Downloading: http://example.python-scraping.com/view/-217\n",
      "Downloading: http://example.python-scraping.com/view/-218\n",
      "Downloading: http://example.python-scraping.com/view/-219\n",
      "Downloading: http://example.python-scraping.com/view/-220\n",
      "Downloading: http://example.python-scraping.com/view/-221\n",
      "Downloading: http://example.python-scraping.com/view/-222\n",
      "Downloading: http://example.python-scraping.com/view/-223\n",
      "Downloading: http://example.python-scraping.com/view/-224\n",
      "Downloading: http://example.python-scraping.com/view/-225\n",
      "Downloading: http://example.python-scraping.com/view/-226\n",
      "Downloading: http://example.python-scraping.com/view/-227\n",
      "Downloading: http://example.python-scraping.com/view/-228\n",
      "Downloading: http://example.python-scraping.com/view/-229\n",
      "Downloading: http://example.python-scraping.com/view/-230\n",
      "Downloading: http://example.python-scraping.com/view/-231\n",
      "Downloading: http://example.python-scraping.com/view/-232\n",
      "Downloading: http://example.python-scraping.com/view/-233\n",
      "Downloading: http://example.python-scraping.com/view/-234\n",
      "Downloading: http://example.python-scraping.com/view/-235\n",
      "Downloading: http://example.python-scraping.com/view/-236\n",
      "Downloading: http://example.python-scraping.com/view/-237\n",
      "Downloading: http://example.python-scraping.com/view/-238\n",
      "Downloading: http://example.python-scraping.com/view/-239\n",
      "Downloading: http://example.python-scraping.com/view/-240\n",
      "Downloading: http://example.python-scraping.com/view/-241\n",
      "Downloading: http://example.python-scraping.com/view/-242\n",
      "Downloading: http://example.python-scraping.com/view/-243\n",
      "Downloading: http://example.python-scraping.com/view/-244\n",
      "Downloading: http://example.python-scraping.com/view/-245\n",
      "Downloading: http://example.python-scraping.com/view/-246\n",
      "Downloading: http://example.python-scraping.com/view/-247\n",
      "Download error: NOT FOUND\n"
     ]
    }
   ],
   "source": [
    "crawl_site('http://example.python-scraping.com/view/-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这段代码中对ID进行遍历，直到出现下载错误再停止，假设抓取已到达最后一个国家的页面。  \n",
    "\n",
    "但这种实现方式是有缺陷的，那就是某些记录可能已被删除，数据库ID之间并不是连续的。这个时候只要访问到某个间隔点，爬虫就会马上退出。以下是这段代码的改进版本，在这个版本中连续发生多次下载错误后才会退出程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_site(url,max_errors = 5):\n",
    "    for page in itertools.count(1):\n",
    "        pg_url = '{}{}'.format(url,page)\n",
    "        html = download(pg_url)\n",
    "        if html is None:\n",
    "            num_errors += 1\n",
    "            if num_errors == max_eerrors:\n",
    "                #max errors reached,exit loop\n",
    "                break\n",
    "        else:\n",
    "            num_errors = 0\n",
    "            #success - can scrape the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码中实现的爬虫得连续5次下载错误才会停止遍历，这样就极大地降低了遇到记录被删除或隐藏时过早停止遍历的风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在爬取网站时，遍历ID是一个很便捷的方法，但是和网站地图爬虫一样，这种方法也无法保证始终可用。比如，一些网站会检查页面别名是否在URL中，如果不是，则会返回404Not Found错误。而另一些网站则会使用非连续大数作为ID，或是不使用数值作为ID,此时遍历就难以发挥其作用了。例如，Amazon使用ISBN作为可用图书的ID,这种编码包含至少10位数宇。使用ID对ISBN进行遍历需要测试数十亿次可能的组合，因此这种方法肯定不是抓取该站内害最高效的方法。  \n",
    "正如你一直关注的那样，你可能已经注意到一些TOO MANY REQUESTS下载错误信息。现在无须担心它，我们将会在1.5节的\"高级功能\"部分中介绍更多处理该类型错误的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七、链接爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只要这两种技术可用，就应当使用它们进行自取，因为这两种方法将需要下载的网页数量降至最低。不过，对于另一些网站，我们需要让爬虫表现得更像普通用户，跟踪链接，访问感兴趣的内容。  \n",
    "通过跟踪每个链接的方式，我们可以很容易地下载整个网站的页面。但是，这种方法可能会下载很多并不需要的网页。例如，我们想要从一个在线论坛中抓取用户账号详情页，那么此则我们只需要下载账号页，而不需要下载讨论贴的页面。本文使用的链接爬虫将使用正则表达式来确定应当下载哪些页面。下面是这段代码的初始版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def link_crawler(start_url,link_regex):\n",
    "    \"\"\"Crawl from the given start URL fallowing links matches by link_regex\n",
    "    \"\"\"\n",
    "    crawl_queue = [start_url]\n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        html = download(url)\n",
    "        if html is not None:\n",
    "            continue\n",
    "        #filteer for links matching our regular expression\n",
    "        for link in get_links(html):\n",
    "            if re.match(link_reegex,link):\n",
    "                crawl_queue.append(link)\n",
    "\n",
    "def get_links(html):\n",
    "    \"\"\"Return a list of links from html\n",
    "    \"\"\"\n",
    "    #a reegular expression to eextract all links from the webpage\n",
    "    webpage_regex = re.compile(\"\"\"<a[^>]+herf=[\"'](.*?)[\"']\"\"\",re.IGNORECASE)\n",
    "    #list of all links from the webpage\n",
    "    return webpage_regex.findall(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要运行这段代码，只需要调用link_crawler函数，并传入两个参数。  \n",
    "\n",
    "要爬取的网站URL  \n",
    "\n",
    "用于匹配你想跟踪的链接的正则表达式  \n",
    "\n",
    "对于示例网站来说，我们想要爬取的是国家（或地区）列表索引页和国家（或地区）页面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们查看站点可以得知索引页链接遵循如下格式  \n",
    "http://example.python-scraping.com/index/1  \n",
    "http://example.python-scraping.com/index/2  \n",
    "国家(或地区)页遵循如下格式  \n",
    "http://example.python-scraping.com/view/Afghanistan-1  \n",
    "http://example.python-scraping.com/view/Aland-Islands-2    \n",
    "因此，我们可以用/(index|view)/这个简单的正则表达式来匹配这两类网页。当爬虫使用这些输入参数运行时会发生什么呢?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你会得到如下所示的下载错误："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.python-scraping.com\n"
     ]
    }
   ],
   "source": [
    "link_crawler('http://example.python-scraping.com','/(index|view)/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，问题出在下载/index/1时，链接只有网页的路径部分没有协议和服务器部分，这是一个相对链接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于浏览器知道你正在浏览哪个网页，并且能够采取必要的步骤处理这些链接，因此在浏览器浏览时，相对链接是能够正常工作的。但是，urllib并没有上下文。为了让urllib能够定位网页，我们需要将链接转推为绝对链接的形式，以便包含定位网页的所有细节。如你所愿，Python的urllib中有一个横块可以用来实现该功能，该横块名为parse。下面是link_crawler的改进板本，使用了urljoin方法来创建绝对路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "def link_craeler(start_url,link_regex):\n",
    "    \"\"\"Crawl from the given start URL following links matched by link_regex\n",
    "    \"\"\"\n",
    "    crawl_queue = [start_url]\n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        html = download(url)\n",
    "        if not html:\n",
    "            continue\n",
    "        for link in get_links(html):\n",
    "            if re.match(link_regex,link):\n",
    "                abs_link = urljoin(start_url,link)\n",
    "                crawl_queue.append(abs_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在运行这段代码时，虽然下载了匹配的网页，但是同样的地点会被不断重复下载到。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "产生该行为的原因是这些地点相互之间存在链接。  \n",
    "比如，澳大利亚链接到了南极洲，而南极洲又链接回了澳大利亚，此时爬虫就会继续将这些URL放入队列，永远不会到达队列尾部。要想避免重复爬取相同的链接，我们需要记录哪些链接已经被爬取过。下面是修改后的link_crawler函数，具备了存储已发现URL的功能，可以避免重复下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_crawler(start_url,link_regex):\n",
    "    crawl_queue = [start_url]\n",
    "    #keep track which URL's have seen before\n",
    "    seen = set(crawl_queue)\n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        html = download(url)\n",
    "        if not html:\n",
    "            continue\n",
    "        for link in get_links(html):\n",
    "            #check if link matches expected regex\n",
    "            if re.match(link_regex,link):\n",
    "                abs_link = urljoin(start_url,link)\n",
    "            #check if have already seen this link\n",
    "            if abs_link not in seen:\n",
    "                seen.add(abs_link)\n",
    "                crawl_queue.append(abs_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们运行这个脚本的时候它会爬取所有地点，并且能够如期停止。最终得到了一个可用的链接爬虫。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
